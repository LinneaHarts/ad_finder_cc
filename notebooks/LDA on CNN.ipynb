{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on CNN Corpus #\n",
    "\n",
    "In this notebook, I will run Latent Dirichalet Allocation on the CNN sentences to model topics.\n",
    "\n",
    "Experiment: LDA on snippets vs. sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim, spacy\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346045"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df = pd.read_csv('../data/interim/cnn-last-year-sent-comb.csv')\n",
    "cnn_df, _ = train_test_split(cnn_df.drop(columns=['Unnamed: 0', \n",
    "                                                 'Unnamed: 0.1',\n",
    "                                                'Unnamed: 0.1.1']).dropna(), test_size=0.9, random_state=18)\n",
    "len(cnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could',\n",
    "                           '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many',\n",
    "                           'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily',\n",
    "                           'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right',\n",
    "                           'line', 'even', 'also', 'may', 'take', 'come', 'hi', 'ha', 'le', 'u', 'wa', 'thi',\n",
    "                           'to', 'one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(\"([\\d,\\,\\./!#$%&\\'\\\":;>\\?@\\[\\]`)(\\+])+\", \"\", sent) # remove digits and remove punctuation\n",
    "        sent = re.sub(\"([-])+\", \" \", sent)\n",
    "        sent = simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 346045 entries, 348928 to 2450552\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   sentence     346045 non-null  object\n",
      " 1   start_snip   346045 non-null  int64 \n",
      " 2   end_snip     346045 non-null  int64 \n",
      " 3   contributor  346045 non-null  object\n",
      " 4   runtime      346045 non-null  object\n",
      " 5   start_time   346045 non-null  object\n",
      " 6   stop_time    346045 non-null  object\n",
      " 7   identifier   346045 non-null  object\n",
      " 8   subjects     346045 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 26.4+ MB\n"
     ]
    }
   ],
   "source": [
    "cnn_df = cnn_df.dropna()\n",
    "cnn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cnn_df.sentence.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346045"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "len(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably high'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corpus = [' '.join(data_ready[i]) for i in range(len(data_ready))]\n",
    "X_corpus[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words=stop_words, min_df=2, max_df=0.3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(X_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=75, n_jobs=-1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 75\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "turn news scary credit mind distance unlimited tremendous stake forever\n",
      "\n",
      "Topic #1:\n",
      "agree thought election testify terrible represent allergic healthy situation excellent\n",
      "\n",
      "Topic #2:\n",
      "truth move different viewer strong switch street charge miss debate\n",
      "\n",
      "Topic #3:\n",
      "win minute conversation able best stock policy consistent day able help\n",
      "\n",
      "Topic #4:\n",
      "deal month course protein negative last month administration virus last month ago\n",
      "\n",
      "Topic #5:\n",
      "trust accept huge bottom racist white criticize severe people intend\n",
      "\n",
      "Topic #6:\n",
      "watch stay everywhere elect touch look closely awful music fall\n",
      "\n",
      "Topic #7:\n",
      "way vote plan describe apologize audience effective holiday guilty decline\n",
      "\n",
      "Topic #8:\n",
      "next tonight protect like concern perfect write next week coverage set\n",
      "\n",
      "Topic #9:\n",
      "love occur car bus work sanction really build body assume\n",
      "\n",
      "Topic #10:\n",
      "time care bad appreciate first time information stuff spend arrest second\n",
      "\n",
      "Topic #11:\n",
      "end expect suppose fact study recover sensitive speed spin average\n",
      "\n",
      "Topic #12:\n",
      "home place little stay home definitely incredible stay brand wear let bring\n",
      "\n",
      "Topic #13:\n",
      "night last last night bring world beautiful comment busy arrive angry\n",
      "\n",
      "Topic #14:\n",
      "day give state support probably quick color damage important hang\n",
      "\n",
      "Topic #15:\n",
      "start simple imagine speak part job challenge former eliquis video\n",
      "\n",
      "Topic #16:\n",
      "matter lead opinion compare whole internet actor fresh concentration political\n",
      "\n",
      "Topic #17:\n",
      "people work change surprised partner child people work impossible impeachable appeal\n",
      "\n",
      "Topic #18:\n",
      "long feel long time ago time week ago lay okay front week\n",
      "\n",
      "Topic #19:\n",
      "name reality significant dangerous trouble people insulin negotiation band resign\n",
      "\n",
      "Topic #20:\n",
      "wrong true new evidence moderate goal neighborhood happen new direction happen\n",
      "\n",
      "Topic #21:\n",
      "case understand find fair pill shop blame week passenger requirement\n",
      "\n",
      "Topic #22:\n",
      "begin hold sick son note later almost people die inside die\n",
      "\n",
      "Topic #23:\n",
      "wait witness campaign decide fire exercise bill move forward cost hide\n",
      "\n",
      "Topic #24:\n",
      "today liberty save visit happy quickly nausea_vomite obviously call call today\n",
      "\n",
      "Topic #25:\n",
      "absolutely test real friend include deny catch extraordinary people piece\n",
      "\n",
      "Topic #26:\n",
      "detail recognize illegal overnight neighbor staff chief destroy otherwise plot\n",
      "\n",
      "Topic #27:\n",
      "pay buy stop attention beat vice ever low ability town\n",
      "\n",
      "Topic #28:\n",
      "answer question answer question respect context awesome scare time involve simple awesome\n",
      "\n",
      "Topic #29:\n",
      "question ahead great story moment eye laugh forget reporting sister\n",
      "\n",
      "Topic #30:\n",
      "reporter maybe room dead pain anymore schedule solution lesson window\n",
      "\n",
      "Topic #31:\n",
      "ask live first issue ask question crazy update question game pleasure\n",
      "\n",
      "Topic #32:\n",
      "year exactly lie year ago last year ago investigate sign book last\n",
      "\n",
      "Topic #33:\n",
      "late discuss idea none much attorney much time scale dollar small\n",
      "\n",
      "Topic #34:\n",
      "picture depend drop network person boy available city unbelievable bear\n",
      "\n",
      "Topic #35:\n",
      "report serious continue hate sale responsible perspective waist introduce hopeful\n",
      "\n",
      "Topic #36:\n",
      "process discover enemy foreign opening excited confidence society save life isolate\n",
      "\n",
      "Topic #37:\n",
      "week politic free careful luck increase engineer personal react currently\n",
      "\n",
      "Topic #38:\n",
      "unfortunately away alone gun unusual literally lawyer honestly back bring back\n",
      "\n",
      "Topic #39:\n",
      "kill side meet remarkable ambassador people effect criticism normally compromise\n",
      "\n",
      "Topic #40:\n",
      "look listen never president meeting hearing race baby date wonderful\n",
      "\n",
      "Topic #41:\n",
      "talk tomorrow farmer cut let talk cry ignore people opportunity governor\n",
      "\n",
      "Topic #42:\n",
      "play power fine investigation testing abuse safe sander scene seat\n",
      "\n",
      "Topic #43:\n",
      "remember sit well hand proof aware hot difficult clean chance\n",
      "\n",
      "Topic #44:\n",
      "guy problem check break evening tv admit serve early mine\n",
      "\n",
      "Topic #45:\n",
      "keep mean office movie afraid remove threat critical various double\n",
      "\n",
      "Topic #46:\n",
      "read war age weird finish hero incident gas shooting vitamin_aisle\n",
      "\n",
      "Topic #47:\n",
      "welcome figure explain wonder kid open girl forth become journalist\n",
      "\n",
      "Topic #48:\n",
      "call phone impeachment phone call article prepare president article impeachment opposite joke\n",
      "\n",
      "Topic #49:\n",
      "certainly future sound forward afternoon flu season indeed common risk\n",
      "\n",
      "Topic #50:\n",
      "let family experience rest event share refer operation land price\n",
      "\n",
      "Topic #51:\n",
      "much join tough ready business much join face mistake sad trump\n",
      "\n",
      "Topic #52:\n",
      "correct argument guess actually step dog high patient reopen ground\n",
      "\n",
      "Topic #53:\n",
      "reason always number old respond wife choose year old well life worth\n",
      "\n",
      "Topic #54:\n",
      "learn mention hour reward correspondent help keep candidate protest jump help\n",
      "\n",
      "Topic #55:\n",
      "cover worry finally worried medicine attack fight fun page feeling\n",
      "\n",
      "Topic #56:\n",
      "control seriously crime doubt count travel anywhere pro glad deserve\n",
      "\n",
      "Topic #57:\n",
      "morning sense man put shoot brother draw happen drug bed\n",
      "\n",
      "Topic #58:\n",
      "happen yet still choice american period rule american people progress people\n",
      "\n",
      "Topic #59:\n",
      "school follow head surprise delegate image relationship tape disaster panic\n",
      "\n",
      "Topic #60:\n",
      "important limited proud energy tweet limited time approach time sleep mission\n",
      "\n",
      "Topic #61:\n",
      "thing interesting reaction little_bit discussion trial big deal fly differently big\n",
      "\n",
      "Topic #62:\n",
      "back leave movement decision honest push comfortable either apparently raise\n",
      "\n",
      "Topic #63:\n",
      "tell doctor point word enough ask doctor tell doctor response ask far\n",
      "\n",
      "Topic #64:\n",
      "hear help else water member return people eventually backing veteran\n",
      "\n",
      "Topic #65:\n",
      "message guarantee show approve walk democratic eat hurt presidential deliver\n",
      "\n",
      "Topic #66:\n",
      "stand together phase example yesterday crash history advice plant delay\n",
      "\n",
      "Topic #67:\n",
      "parent couple prove send let track daughter enjoy poll normal\n",
      "\n",
      "Topic #68:\n",
      "life weekend money statement factor shock false million judge danger\n",
      "\n",
      "Topic #69:\n",
      "really difference hard big country promise sorry young impeach confirm\n",
      "\n",
      "Topic #70:\n",
      "sure possible defense woman handle totally different fail consider shot\n",
      "\n",
      "Topic #71:\n",
      "believe lose amazing people mile mad action hope tie talk\n",
      "\n",
      "Topic #72:\n",
      "mom option die stick datum least notice cool obvious photo\n",
      "\n",
      "Topic #73:\n",
      "pass soon clear health secret public health divide public month real\n",
      "\n",
      "Topic #74:\n",
      "concerned strength disagree ride advantage death address people church pretty\n"
     ]
    }
   ],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda, open('../models/cnn_lda.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "      <th>topic_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372137</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563   \n",
       "\n",
       "    topic_7   topic_8   topic_9  ...  topic_65  topic_66  topic_67  topic_68  \\\n",
       "0  0.003764  0.003764  0.003764  ...  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.004601  0.004601  ...  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.003968  0.003968  0.003968  ...  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  ...  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  ...  0.372137  0.003563  0.003563  0.003563   \n",
       "\n",
       "   topic_69  topic_70  topic_71  topic_72  topic_73  topic_74  \n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  \n",
       "1  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601  \n",
       "2  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968  \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['topic_' + str(i) for i in range(75)]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dom_topic</th>\n",
       "      <th>topic_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.721435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>0.659506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>0.483123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>0.372137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dom_topic  topic_pct\n",
       "0         18   0.721435\n",
       "1         35   0.659506\n",
       "2         35   0.483123\n",
       "3          0   0.013333\n",
       "4         65   0.372137"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_topic_list = []\n",
    "for i, row in enumerate(results):\n",
    "    dom_topic = -1\n",
    "    topic_pct = 0\n",
    "    for j, val in enumerate(row):\n",
    "        if val > topic_pct:\n",
    "            dom_topic = j\n",
    "            topic_pct = val\n",
    "    dom_topic_list.append({'dom_topic':dom_topic, 'topic_pct': topic_pct})\n",
    "\n",
    "dom_topic_df = pd.DataFrame(dom_topic_list)\n",
    "dom_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_snip</th>\n",
       "      <th>end_snip</th>\n",
       "      <th>contributor</th>\n",
       "      <th>runtime</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>identifier</th>\n",
       "      <th>subjects</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "      <th>topic_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348928</td>\n",
       "      <td>before taking ibrance, tell your doctor if you...</td>\n",
       "      <td>780</td>\n",
       "      <td>840</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2019-06-30 19:00:00</td>\n",
       "      <td>2019-06-30 20:00:58</td>\n",
       "      <td>CNNW_20190630_190000_CNN_Newsroom_With_Fredric...</td>\n",
       "      <td>['trump', 'north korea', 'humira', 'harris', '...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2915667</td>\n",
       "      <td>the army core ils ves very prepared to do as ...</td>\n",
       "      <td>540</td>\n",
       "      <td>600</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2020-03-17 16:00:00</td>\n",
       "      <td>2020-03-17 17:00:58</td>\n",
       "      <td>CNNW_20200317_160000_Inside_Politics</td>\n",
       "      <td>['china', 'boeing', 'burke', 'new york', 'fauc...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633374</td>\n",
       "      <td>but thanks to congress permanently extending t...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1980</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2019-11-03 16:00:00</td>\n",
       "      <td>2019-11-03 17:00:59</td>\n",
       "      <td>CNNW_20191103_160000_Reliable_Sources</td>\n",
       "      <td>['trump', 'brian', 'dovato', 'mark zuckerberg'...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2976157</td>\n",
       "      <td>senator, so glad you can be with me right now.</td>\n",
       "      <td>1320</td>\n",
       "      <td>1380</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2020-03-22 20:00:00</td>\n",
       "      <td>2020-03-22 21:00:59</td>\n",
       "      <td>CNNW_20200322_200000_CNN_Newsroom_With_Fredric...</td>\n",
       "      <td>['paul', 'new york', 'd.c.', 'germany', 'safel...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>492159</td>\n",
       "      <td>reporter: democratic leaders deliberately tai...</td>\n",
       "      <td>960</td>\n",
       "      <td>1020</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:59</td>\n",
       "      <td>2019-07-16 20:00:00</td>\n",
       "      <td>2019-07-16 21:00:59</td>\n",
       "      <td>CNNW_20190716_200000_The_Lead_With_Jake_Tapper</td>\n",
       "      <td>['trump', 'usaa', 'humira', 'white house', 'ir...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372137</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           sentence  start_snip  \\\n",
       "0   348928  before taking ibrance, tell your doctor if you...         780   \n",
       "1  2915667   the army core ils ves very prepared to do as ...         540   \n",
       "2  1633374  but thanks to congress permanently extending t...        1920   \n",
       "3  2976157     senator, so glad you can be with me right now.        1320   \n",
       "4   492159   reporter: democratic leaders deliberately tai...         960   \n",
       "\n",
       "   end_snip contributor   runtime           start_time            stop_time  \\\n",
       "0       840        CNNW  01:00:58  2019-06-30 19:00:00  2019-06-30 20:00:58   \n",
       "1       600        CNNW  01:00:58  2020-03-17 16:00:00  2020-03-17 17:00:58   \n",
       "2      1980        CNNW  01:00:58  2019-11-03 16:00:00  2019-11-03 17:00:59   \n",
       "3      1380        CNNW  01:00:58  2020-03-22 20:00:00  2020-03-22 21:00:59   \n",
       "4      1020        CNNW  01:00:59  2019-07-16 20:00:00  2019-07-16 21:00:59   \n",
       "\n",
       "                                          identifier  \\\n",
       "0  CNNW_20190630_190000_CNN_Newsroom_With_Fredric...   \n",
       "1               CNNW_20200317_160000_Inside_Politics   \n",
       "2              CNNW_20191103_160000_Reliable_Sources   \n",
       "3  CNNW_20200322_200000_CNN_Newsroom_With_Fredric...   \n",
       "4     CNNW_20190716_200000_The_Lead_With_Jake_Tapper   \n",
       "\n",
       "                                            subjects  ...  topic_65  topic_66  \\\n",
       "0  ['trump', 'north korea', 'humira', 'harris', '...  ...  0.003764  0.003764   \n",
       "1  ['china', 'boeing', 'burke', 'new york', 'fauc...  ...  0.004601  0.004601   \n",
       "2  ['trump', 'brian', 'dovato', 'mark zuckerberg'...  ...  0.003968  0.003968   \n",
       "3  ['paul', 'new york', 'd.c.', 'germany', 'safel...  ...  0.013333  0.013333   \n",
       "4  ['trump', 'usaa', 'humira', 'white house', 'ir...  ...  0.372137  0.003563   \n",
       "\n",
       "   topic_67  topic_68  topic_69  topic_70  topic_71  topic_72  topic_73  \\\n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563   \n",
       "\n",
       "   topic_74  \n",
       "0  0.003764  \n",
       "1  0.004601  \n",
       "2  0.003968  \n",
       "3  0.013333  \n",
       "4  0.003563  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_results = cnn_df.reset_index().join(dom_topic_df)\n",
    "cnn_results = cnn_results.join(results_df)\n",
    "cnn_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dom_topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence\n",
       "dom_topic          \n",
       "0             58594\n",
       "1              4130\n",
       "2              3911\n",
       "3              3527\n",
       "4              4182\n",
       "...             ...\n",
       "70             3858\n",
       "71             3972\n",
       "72             3794\n",
       "73             3423\n",
       "74             4867\n",
       "\n",
       "[75 rows x 1 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_results[['sentence', 'dom_topic']].groupby('dom_topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results.to_csv('../data/interim/cnn_lda_results_rand_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
