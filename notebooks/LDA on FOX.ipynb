{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on FOX Corpus #\n",
    "\n",
    "In this notebook, I will run Latent Dirichalet Allocation on the FOX sentences to model topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim, spacy\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox_df = pd.read_csv('../data/interim/cnn-last-year-sent-comb.csv')\n",
    "fox_df, _ = train_test_split(fox_df.drop(columns=['Unnamed: 0', \n",
    "                                                 'Unnamed: 0.1',\n",
    "                                                'Unnamed: 0.1.1']).dropna(), test_size=0.9, random_state=18)\n",
    "len(fox_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could',\n",
    "                           '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many',\n",
    "                           'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily',\n",
    "                           'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right',\n",
    "                           'line', 'even', 'also', 'may', 'take', 'come', 'hi', 'ha', 'le', 'u', 'wa', 'thi',\n",
    "                           'to', 'one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(\"([\\d,\\,\\./!#$%&\\'\\\":;>\\?@\\[\\]`)(\\+])+\", \"\", sent) # remove digits and remove punctuation\n",
    "        sent = re.sub(\"([-])+\", \" \", sent)\n",
    "        sent = simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 346045 entries, 348928 to 2450552\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   sentence     346045 non-null  object\n",
      " 1   start_snip   346045 non-null  int64 \n",
      " 2   end_snip     346045 non-null  int64 \n",
      " 3   contributor  346045 non-null  object\n",
      " 4   runtime      346045 non-null  object\n",
      " 5   start_time   346045 non-null  object\n",
      " 6   stop_time    346045 non-null  object\n",
      " 7   identifier   346045 non-null  object\n",
      " 8   subjects     346045 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 26.4+ MB\n"
     ]
    }
   ],
   "source": [
    "fox_df = fox_df.dropna()\n",
    "fox_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fox_df.sentence.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346045"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "len(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably high'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corpus = [' '.join(data_ready[i]) for i in range(len(data_ready))]\n",
    "X_corpus[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words=stop_words, min_df=2, max_df=0.3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(X_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=75, n_jobs=-1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 75\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "election prove perspective discover people die call rid people die call today\n",
      "\n",
      "Topic #1:\n",
      "work bring long find long time time face eat let bring television\n",
      "\n",
      "Topic #2:\n",
      "tonight month concern fast glad last month hope month ago anyway confuse\n",
      "\n",
      "Topic #3:\n",
      "morning agree occur learn politic luck pretty criticism passenger ultimately\n",
      "\n",
      "Topic #4:\n",
      "side finally shop associate compare move forward car effective move depression\n",
      "\n",
      "Topic #5:\n",
      "really hear true tomorrow let continue office healthy really important let talk\n",
      "\n",
      "Topic #6:\n",
      "people big check much much join join inside personally praise quit\n",
      "\n",
      "Topic #7:\n",
      "much hold old surprised proud advantage together year old hot year\n",
      "\n",
      "Topic #8:\n",
      "today let different president count foot protection relief however helpful\n",
      "\n",
      "Topic #9:\n",
      "leave yesterday indeed careful audience unbelievable nausea_vomite travel practice enemy\n",
      "\n",
      "Topic #10:\n",
      "listen important trust decision moderate camera fun feeling bear loss\n",
      "\n",
      "Topic #11:\n",
      "like still definitely dangerous perfect close thing divide husband recommendation\n",
      "\n",
      "Topic #12:\n",
      "talk test health public clear people investigation public health eventually remarkable\n",
      "\n",
      "Topic #13:\n",
      "watch thing day fine read baby price alone picture never\n",
      "\n",
      "Topic #14:\n",
      "evening idea pass already doubt wall attorney exist church involve\n",
      "\n",
      "Topic #15:\n",
      "start give truth wonder early lawyer relationship movie hurt monitor\n",
      "\n",
      "Topic #16:\n",
      "energy strength handle admit false worth mission defend survive strength energy\n",
      "\n",
      "Topic #17:\n",
      "message viewer approve united_state though around approve message welcome viewer world analyst\n",
      "\n",
      "Topic #18:\n",
      "discuss forget aware wife scary operation table law wake cool\n",
      "\n",
      "Topic #19:\n",
      "pay friend play expect schedule date protest drop hearing window\n",
      "\n",
      "Topic #20:\n",
      "year live word bad visit strong follow last year last reality\n",
      "\n",
      "Topic #21:\n",
      "reason family cover head reward medicine book apologize patient work\n",
      "\n",
      "Topic #22:\n",
      "guy understand week sense argument worried last week walk last well\n",
      "\n",
      "Topic #23:\n",
      "move stuff time spend double impossible goal much time wonderful much\n",
      "\n",
      "Topic #24:\n",
      "man kid mom happy ever network straight recognize draw criticize\n",
      "\n",
      "Topic #25:\n",
      "look night last number last night woman possible guess thing proof\n",
      "\n",
      "Topic #26:\n",
      "happen break brand positive eliquis confident represent behind consequence achievable\n",
      "\n",
      "Topic #27:\n",
      "tell vice support testing vice president president internet image capacity let look\n",
      "\n",
      "Topic #28:\n",
      "reporter home explain town honest land fail mayor mother tell\n",
      "\n",
      "Topic #29:\n",
      "none approach send corner big deal personal big independent thing study\n",
      "\n",
      "Topic #30:\n",
      "never correct issue matter guarantee example pill afternoon longer shoot\n",
      "\n",
      "Topic #31:\n",
      "answer difference state better little_bit concerned government lesson appreciate time opposite\n",
      "\n",
      "Topic #32:\n",
      "point ahead speak period time virus describe stage important clean\n",
      "\n",
      "Topic #33:\n",
      "beat race respond laugh person fly plane democratic presidential invite\n",
      "\n",
      "Topic #34:\n",
      "ready trial son impeachment favor trouble impeachment trial arm investigation area\n",
      "\n",
      "Topic #35:\n",
      "time believe keep first time control parent first sad obvious space\n",
      "\n",
      "Topic #36:\n",
      "maybe everywhere sorry depend crime lead high lemon note service\n",
      "\n",
      "Topic #37:\n",
      "wrong wait welcome minute campaign step quickly credit start fair\n",
      "\n",
      "Topic #38:\n",
      "call stand phone begin phone call candidate actually focus boy crash\n",
      "\n",
      "Topic #39:\n",
      "unfortunately course pro folk voice back work ought people journalist\n",
      "\n",
      "Topic #40:\n",
      "life save stay thought die best elect hang reach save life\n",
      "\n",
      "Topic #41:\n",
      "american american people article impeachment tough chance people article impeachment farmer delegate\n",
      "\n",
      "Topic #42:\n",
      "exactly option list switch anymore benefit choose cost kind otezla\n",
      "\n",
      "Topic #43:\n",
      "question lie moment part answer answer question phase evidence arrest later\n",
      "\n",
      "Topic #44:\n",
      "join power yet money end water abuse food real flu\n",
      "\n",
      "Topic #45:\n",
      "lose late safe ask doctor doctor game young ask protein mad\n",
      "\n",
      "Topic #46:\n",
      "great response respect opinion impeach history development anywhere similar possibility\n",
      "\n",
      "Topic #47:\n",
      "doctor first tell doctor world tell ago year ago symptom business partner\n",
      "\n",
      "Topic #48:\n",
      "free put crazy include eye shape extraordinary available react celebrate\n",
      "\n",
      "Topic #49:\n",
      "amazing afraid question ask question conversation outside ask comfortable age sometimes\n",
      "\n",
      "Topic #50:\n",
      "war ventilator datum progress performance mess feel differently death datum plan\n",
      "\n",
      "Topic #51:\n",
      "remember news beautiful discussion reporting factor chief program agent gift\n",
      "\n",
      "Topic #52:\n",
      "hour show movement hand match smart heart model less necessary\n",
      "\n",
      "Topic #53:\n",
      "meet disagree dog source daughter opening leak victory unclear reliable\n",
      "\n",
      "Topic #54:\n",
      "next love experience next week former week share context ground ride\n",
      "\n",
      "Topic #55:\n",
      "figure well imagine ability low keep infection risk help keep busy\n",
      "\n",
      "Topic #56:\n",
      "sure weekend sign sound surprise quick miss party thing engineer\n",
      "\n",
      "Topic #57:\n",
      "promise comment buy incredible catch angry tape population day attention\n",
      "\n",
      "Topic #58:\n",
      "else fact strategy interest director finish scene new day ship day\n",
      "\n",
      "Topic #59:\n",
      "care vote place member seriously deny meeting terrible responsibility house\n",
      "\n",
      "Topic #60:\n",
      "back reaction suppose week second week ago hide ago couple difficult\n",
      "\n",
      "Topic #61:\n",
      "mention mind fight probably kill seat taxis ad action fully\n",
      "\n",
      "Topic #62:\n",
      "ask feel enough change witness significant limited hate limited time sander\n",
      "\n",
      "Topic #63:\n",
      "problem fire okay pleasure ice process various consistent almost scale\n",
      "\n",
      "Topic #64:\n",
      "simple country serious blame awesome tweet simple awesome introduce girl illegal\n",
      "\n",
      "Topic #65:\n",
      "stop protect debate pain statement victim team totally constipation coverage\n",
      "\n",
      "Topic #66:\n",
      "way deal appreciate stay home set stay far home applause order\n",
      "\n",
      "Topic #67:\n",
      "certainly plan worry room testify lay refer challenge apply drug\n",
      "\n",
      "Topic #68:\n",
      "situation able bottom brief economy heat company predict help tower\n",
      "\n",
      "Topic #69:\n",
      "new write turn job accept fix rain investigate happen new happen\n",
      "\n",
      "Topic #70:\n",
      "stick little season store signal corrupt reflect nervous target dozen\n",
      "\n",
      "Topic #71:\n",
      "liberty story sit interesting win information soon accident secret weird\n",
      "\n",
      "Topic #72:\n",
      "absolutely brother attack notice suspect scare open stress music least\n",
      "\n",
      "Topic #73:\n",
      "help report name mean hard detail away choice mistake decide\n",
      "\n",
      "Topic #74:\n",
      "case always jump charge prepare event crowd joke realize unusual\n"
     ]
    }
   ],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda, open('../models/fox_lda.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "      <th>topic_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.659506</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.706399</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.659506  0.004601  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.706399  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563   \n",
       "\n",
       "    topic_7   topic_8   topic_9  ...  topic_65  topic_66  topic_67  topic_68  \\\n",
       "0  0.003764  0.003764  0.003764  ...  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.004601  0.004601  ...  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.003968  0.003968  0.003968  ...  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  ...  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  ...  0.003563  0.168263  0.003563  0.003563   \n",
       "\n",
       "   topic_69  topic_70  topic_71  topic_72  topic_73  topic_74  \n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  \n",
       "1  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601  \n",
       "2  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968  \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['topic_' + str(i) for i in range(75)]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dom_topic</th>\n",
       "      <th>topic_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>0.721435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.659506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.706399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0.433572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dom_topic  topic_pct\n",
       "0         55   0.721435\n",
       "1          1   0.659506\n",
       "2          0   0.706399\n",
       "3          0   0.013333\n",
       "4         40   0.433572"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_topic_list = []\n",
    "for i, row in enumerate(results):\n",
    "    dom_topic = -1\n",
    "    topic_pct = 0\n",
    "    for j, val in enumerate(row):\n",
    "        if val > topic_pct:\n",
    "            dom_topic = j\n",
    "            topic_pct = val\n",
    "    dom_topic_list.append({'dom_topic':dom_topic, 'topic_pct': topic_pct})\n",
    "\n",
    "dom_topic_df = pd.DataFrame(dom_topic_list)\n",
    "dom_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_snip</th>\n",
       "      <th>end_snip</th>\n",
       "      <th>contributor</th>\n",
       "      <th>runtime</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>identifier</th>\n",
       "      <th>subjects</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "      <th>topic_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348928</td>\n",
       "      <td>before taking ibrance, tell your doctor if you...</td>\n",
       "      <td>780</td>\n",
       "      <td>840</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2019-06-30 19:00:00</td>\n",
       "      <td>2019-06-30 20:00:58</td>\n",
       "      <td>CNNW_20190630_190000_CNN_Newsroom_With_Fredric...</td>\n",
       "      <td>['trump', 'north korea', 'humira', 'harris', '...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2915667</td>\n",
       "      <td>the army core ils ves very prepared to do as ...</td>\n",
       "      <td>540</td>\n",
       "      <td>600</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2020-03-17 16:00:00</td>\n",
       "      <td>2020-03-17 17:00:58</td>\n",
       "      <td>CNNW_20200317_160000_Inside_Politics</td>\n",
       "      <td>['china', 'boeing', 'burke', 'new york', 'fauc...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633374</td>\n",
       "      <td>but thanks to congress permanently extending t...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1980</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2019-11-03 16:00:00</td>\n",
       "      <td>2019-11-03 17:00:59</td>\n",
       "      <td>CNNW_20191103_160000_Reliable_Sources</td>\n",
       "      <td>['trump', 'brian', 'dovato', 'mark zuckerberg'...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2976157</td>\n",
       "      <td>senator, so glad you can be with me right now.</td>\n",
       "      <td>1320</td>\n",
       "      <td>1380</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:58</td>\n",
       "      <td>2020-03-22 20:00:00</td>\n",
       "      <td>2020-03-22 21:00:59</td>\n",
       "      <td>CNNW_20200322_200000_CNN_Newsroom_With_Fredric...</td>\n",
       "      <td>['paul', 'new york', 'd.c.', 'germany', 'safel...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>492159</td>\n",
       "      <td>reporter: democratic leaders deliberately tai...</td>\n",
       "      <td>960</td>\n",
       "      <td>1020</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>01:00:59</td>\n",
       "      <td>2019-07-16 20:00:00</td>\n",
       "      <td>2019-07-16 21:00:59</td>\n",
       "      <td>CNNW_20190716_200000_The_Lead_With_Jake_Tapper</td>\n",
       "      <td>['trump', 'usaa', 'humira', 'white house', 'ir...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           sentence  start_snip  \\\n",
       "0   348928  before taking ibrance, tell your doctor if you...         780   \n",
       "1  2915667   the army core ils ves very prepared to do as ...         540   \n",
       "2  1633374  but thanks to congress permanently extending t...        1920   \n",
       "3  2976157     senator, so glad you can be with me right now.        1320   \n",
       "4   492159   reporter: democratic leaders deliberately tai...         960   \n",
       "\n",
       "   end_snip contributor   runtime           start_time            stop_time  \\\n",
       "0       840        CNNW  01:00:58  2019-06-30 19:00:00  2019-06-30 20:00:58   \n",
       "1       600        CNNW  01:00:58  2020-03-17 16:00:00  2020-03-17 17:00:58   \n",
       "2      1980        CNNW  01:00:58  2019-11-03 16:00:00  2019-11-03 17:00:59   \n",
       "3      1380        CNNW  01:00:58  2020-03-22 20:00:00  2020-03-22 21:00:59   \n",
       "4      1020        CNNW  01:00:59  2019-07-16 20:00:00  2019-07-16 21:00:59   \n",
       "\n",
       "                                          identifier  \\\n",
       "0  CNNW_20190630_190000_CNN_Newsroom_With_Fredric...   \n",
       "1               CNNW_20200317_160000_Inside_Politics   \n",
       "2              CNNW_20191103_160000_Reliable_Sources   \n",
       "3  CNNW_20200322_200000_CNN_Newsroom_With_Fredric...   \n",
       "4     CNNW_20190716_200000_The_Lead_With_Jake_Tapper   \n",
       "\n",
       "                                            subjects  ...  topic_65  topic_66  \\\n",
       "0  ['trump', 'north korea', 'humira', 'harris', '...  ...  0.003764  0.003764   \n",
       "1  ['china', 'boeing', 'burke', 'new york', 'fauc...  ...  0.004601  0.004601   \n",
       "2  ['trump', 'brian', 'dovato', 'mark zuckerberg'...  ...  0.003968  0.003968   \n",
       "3  ['paul', 'new york', 'd.c.', 'germany', 'safel...  ...  0.013333  0.013333   \n",
       "4  ['trump', 'usaa', 'humira', 'white house', 'ir...  ...  0.003563  0.168263   \n",
       "\n",
       "   topic_67  topic_68  topic_69  topic_70  topic_71  topic_72  topic_73  \\\n",
       "0  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764  0.003764   \n",
       "1  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601  0.004601   \n",
       "2  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968  0.003968   \n",
       "3  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333  0.013333   \n",
       "4  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563  0.003563   \n",
       "\n",
       "   topic_74  \n",
       "0  0.003764  \n",
       "1  0.004601  \n",
       "2  0.003968  \n",
       "3  0.013333  \n",
       "4  0.003563  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox_results = fox_df.reset_index().join(dom_topic_df)\n",
    "fox_results = fox_results.join(results_df)\n",
    "fox_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dom_topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence\n",
       "dom_topic          \n",
       "0             57665\n",
       "1              3969\n",
       "2              4623\n",
       "3              3632\n",
       "4              4290\n",
       "...             ...\n",
       "70             3721\n",
       "71             3687\n",
       "72             3746\n",
       "73             4584\n",
       "74             3682\n",
       "\n",
       "[75 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox_results[['sentence', 'dom_topic']].groupby('dom_topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_results.to_csv('../data/interim/fox_lda_results_rand_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
