{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Best FOX Models #\n",
    "\n",
    "In this notebook, I will select the top three best models and tune their hyperparameters to get the best fit.\n",
    "\n",
    "| Model | Accuracy |\n",
    "| ---- | ---- |\n",
    "| LogisticRegression | 0.867 | \n",
    "| KNeighborsClassifier | 0.863 | \n",
    "| SVC | 0.844 | \n",
    "| LinearSVC | 0.865 | \n",
    "| SGDClassifier | 0.842 | \n",
    "| DecisionTreeClassifier | 0.874| \n",
    "| RandomForestClassifier | 0.891 | \n",
    "| BaggingClassifier | 0.899 | \n",
    "| GradientBoostingClassifier | 0.880 | \n",
    "| AdaBoostClassifier | 0.878 | \n",
    "\n",
    "GradientBoosting, RandomForest, and Bagging have the best accuracy so these are the ones I will tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>start_snip</th>\n",
       "      <th>end_snip</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cluster</th>\n",
       "      <th>ad_cluster</th>\n",
       "      <th>news_cluster</th>\n",
       "      <th>snip_ad</th>\n",
       "      <th>isad</th>\n",
       "      <th>contributor</th>\n",
       "      <th>...</th>\n",
       "      <th>has_next_back</th>\n",
       "      <th>has_prev_ahead</th>\n",
       "      <th>has_next_good evening</th>\n",
       "      <th>has_next_welcome</th>\n",
       "      <th>has_prev_after this</th>\n",
       "      <th>has_next_good morning</th>\n",
       "      <th>has_prev_applause</th>\n",
       "      <th>has_next_applause</th>\n",
       "      <th>has_prev_tuned</th>\n",
       "      <th>has_prev_go away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>harris: it hasn't been a busy hour.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>glad you are long.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>got to see air force one with the president co...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>keep watching fox news. here is dana.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>dana: your daily briefing starts now.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          identifier  start_snip  end_snip  \\\n",
       "0  FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...           0        60   \n",
       "1  FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...           0        60   \n",
       "2  FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...           0        60   \n",
       "3  FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...           0        60   \n",
       "4  FOXNEWSW_20190528_180000_The_Daily_Briefing_Wi...           0        60   \n",
       "\n",
       "                                            sentence  cluster  ad_cluster  \\\n",
       "0                harris: it hasn't been a busy hour.        2           0   \n",
       "1                                 glad you are long.        2           0   \n",
       "2  got to see air force one with the president co...        2           0   \n",
       "3              keep watching fox news. here is dana.        2           0   \n",
       "4              dana: your daily briefing starts now.        2           0   \n",
       "\n",
       "   news_cluster  snip_ad  isad contributor  ... has_next_back has_prev_ahead  \\\n",
       "0             0        0   0.0    FOXNEWSW  ...             0              0   \n",
       "1             0        0   0.0    FOXNEWSW  ...             0              0   \n",
       "2             0        0   0.0    FOXNEWSW  ...             0              0   \n",
       "3             0        0   0.0    FOXNEWSW  ...             0              0   \n",
       "4             0        0   0.0    FOXNEWSW  ...             0              0   \n",
       "\n",
       "  has_next_good evening has_next_welcome  has_prev_after this  \\\n",
       "0                     0                0                    0   \n",
       "1                     0                0                    0   \n",
       "2                     0                0                    0   \n",
       "3                     0                0                    0   \n",
       "4                     0                0                    0   \n",
       "\n",
       "   has_next_good morning  has_prev_applause  has_next_applause  \\\n",
       "0                      0                  0                  0   \n",
       "1                      0                  0                  0   \n",
       "2                      0                  0                  0   \n",
       "3                      0                  0                  0   \n",
       "4                      0                  0                  0   \n",
       "\n",
       "   has_prev_tuned  has_prev_go away  \n",
       "0               0                 0  \n",
       "1               0                 0  \n",
       "2               0                 0  \n",
       "3               0                 0  \n",
       "4               0                 0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox_df = pd.read_excel('../data/interim/fox_ready_to_code.xlsx').drop(columns=['Unnamed: 0', 'Unnamed: 0.1']).fillna(0)\n",
    "fox_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(fox_df['isad'])\n",
    "X = fox_df.drop(columns=['identifier', 'contributor', 'subjects', 'start_time', 'stop_time', 'runtime',\n",
    "                        'isad']).dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could',\n",
    "                           '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many',\n",
    "                           'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily',\n",
    "                           'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right',\n",
    "                           'line', 'even', 'also', 'may', 'take', 'come', 'hi', 'ha', 'le', 'u', 'wa', 'thi',\n",
    "                           'to', 'one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sent(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = str(sent)\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(\"([\\d,\\,\\./!#$%&\\'\\\":;>\\?@\\[\\]`)(\\+])+\", \"\", sent) # remove digits and remove punctuation\n",
    "        sent = re.sub(\"([-])+\", \" \", sent)\n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they dont want to',\n",
       " 'and as i said ive put on as a lawyer expert witnesses',\n",
       " 'bidens website has been updated with citations',\n",
       " 'all that continues next',\n",
       " ' to build on what john said yes pomp circumstance and some politics which donald trump talked about on his trip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train = list(clean_sent(X_train.sentence.values.tolist()))\n",
    "corpus_test = list(clean_sent(X_test.sentence.values.tolist()))\n",
    "corpus_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize before vectorizing\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = TfidfVectorizer(tokenizer=LemmaTokenizer(), strip_accents='unicode', stop_words='english', \n",
    "                       min_df=2, max_df=0.3, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linne\\Anaconda3\\envs\\ad-finder-cc\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = vect.fit_transform(corpus_train)\n",
    "X_test_bow = vect.transform(corpus_test)\n",
    "X_train_bow_df = pd.DataFrame(X_train_bow.toarray())\n",
    "X_train_joined = X_train.reset_index().join(X_train_bow_df).drop(columns=['index'])\n",
    "X_test_bow_df = pd.DataFrame(X_test_bow.toarray())\n",
    "X_test_joined = X_test.reset_index().join(X_test_bow_df).drop(columns=['index'])\n",
    "X_train_joined = X_train_joined.drop(columns=['sentence'])\n",
    "X_test_joined = X_test_joined.drop(columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    c=confusion_matrix(y, pred)\n",
    "    sns.heatmap(c,cmap='BrBG',annot=True)\n",
    "    print(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion' : ['friedman_mse', 'mse', 'mae'],\n",
    "         'max_depth' : [10, 50, 75, 100, 200],\n",
    "         'min_samples_split': [2, 5, 10]}\n",
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "cv = RandomizedSearchCV(gbc, params)\n",
    "cv.fit(X_train_joined, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test_joined, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cv, X_test_joined, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier(criterion='entropy', random_state=18)\n",
    "params = {'n_estimators': [10, 100, 500, 1000],\n",
    "         'max_depth' : [10, 50, 100, 200],\n",
    "         'max_features' : ['sqrt', 'log2', None],\n",
    "         'min_samples_split': [2, 5, 10]}\n",
    "cv = RandomizedSearchCV(rfc, params, n_iter=6)\n",
    "#cv = GridSearchCV(rfc, params)\n",
    "cv.fit(X_train_joined, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test_joined, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cv, X_test_joined, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc = ensemble.RandomForestClassifier(criterion='entropy', n_estimators=100, min_samples_split=2,\n",
    "#                                      max_features='sqrt', random_state=18)\n",
    "#params = {'max_depth' : [150, 200, 300, 500]}\n",
    "#cv = RandomizedSearchCV(rfc, params)\n",
    "#cv = GridSearchCV(rfc, params)\n",
    "#cv.fit(X_train_joined, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "#print(\"Accuracy: {}\".format(cv.score(X_test_joined, y_test)))\n",
    "#print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That got worse, so let's say these parameters are the best: \n",
    "\n",
    "Tuned Model Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgc = ensemble.BaggingClassifier(random_state=18)\n",
    "params = {'n_estimators': [10, 100, 500, 1000],\n",
    "         'max_features' : [0.25, 0.5, 0.75, 1.0]}\n",
    "cv = RandomizedSearchCV(bgc, params, n_iter=6)\n",
    "cv.fit(X_train_joined, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test_joined, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
